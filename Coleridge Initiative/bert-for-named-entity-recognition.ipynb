{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"bert-for-named-entity-recognition.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ulTz58x6sI8K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621279082177,"user_tz":-240,"elapsed":20518,"user":{"displayName":"Uttkarsh Singh","photoUrl":"https://lh5.googleusercontent.com/--q5kAe7VjVc/AAAAAAAAAAI/AAAAAAAAdGM/KUtIfam9czo/s64/photo.jpg","userId":"16868760332498784389"}},"outputId":"3d5c93e0-6979-42c7-aba1-d2d78f72e0f3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6cofn_M3hx-u"},"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"EaS84mc9h3AR","executionInfo":{"status":"ok","timestamp":1621275165939,"user_tz":-240,"elapsed":765,"user":{"displayName":"Uttkarsh Singh","photoUrl":"https://lh5.googleusercontent.com/--q5kAe7VjVc/AAAAAAAAAAI/AAAAAAAAdGM/KUtIfam9czo/s64/photo.jpg","userId":"16868760332498784389"}},"outputId":"46fa8fe5-466a-47b2-d057-a926f244fc49"},"source":["%cd /content/gdrive/My Drive/Kaggle\n","%pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Kaggle\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/Kaggle'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7pMRlith9N2","executionInfo":{"status":"ok","timestamp":1621276078761,"user_tz":-240,"elapsed":624527,"user":{"displayName":"Uttkarsh Singh","photoUrl":"https://lh5.googleusercontent.com/--q5kAe7VjVc/AAAAAAAAAAI/AAAAAAAAdGM/KUtIfam9czo/s64/photo.jpg","userId":"16868760332498784389"}},"outputId":"2b1584e8-74e6-4bee-cece-98fe5b05c337"},"source":["!kaggle competitions download -c coleridgeinitiative-show-us-the-data\n","!kaggle datasets download -d xujingzhao/apexpytorch\n","!kaggle datasets download -d xhlulu/huggingface-bert\n","!kaggle datasets download -d jonathanbesomi/simple-transformers-pypi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100% 24.2G/24.2G [10:23<00:00, 21.8MB/s]\n","100% 24.2G/24.2G [10:23<00:00, 41.7MB/s]\n","Downloading simple-transformers-pypi.zip to /content/gdrive/My Drive/Kaggle\n","  0% 0.00/141k [00:00<?, ?B/s]\n","100% 141k/141k [00:00<00:00, 19.7MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQeqLKIWipKH"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MeEx8DzSiSJe","executionInfo":{"status":"ok","timestamp":1621276961439,"user_tz":-240,"elapsed":34439,"user":{"displayName":"Uttkarsh Singh","photoUrl":"https://lh5.googleusercontent.com/--q5kAe7VjVc/AAAAAAAAAAI/AAAAAAAAdGM/KUtIfam9czo/s64/photo.jpg","userId":"16868760332498784389"}},"outputId":"ecd1a1e4-4869-4353-a276-05fa36e74438"},"source":["!unzip \\*.zip  && rm *.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  inflating: bert-base-multilingual-cased/tokenizer.json  \n","  inflating: bert-base-multilingual-cased/vocab.txt  \n","  inflating: bert-base-multilingual-uncased/config.json  \n","  inflating: bert-base-multilingual-uncased/modelcard.json  \n","  inflating: bert-base-multilingual-uncased/pytorch_model.bin  \n","bert-base-multilingual-uncased/pytorch_model.bin:  write error (disk full?).  Continue? (y/n/^C) "],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"trusted":true,"id":"zDJJQO3mgk4s"},"source":["!pip install ../input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl\n","!pip install ../input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl\n","!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/apexpytorch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"spuYvCWogk40"},"source":["import os\n","import re\n","import json\n","import time\n","import datetime\n","import random\n","import glob\n","import importlib\n","from functools import partial\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","random.seed(123)\n","np.random.seed(456)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"RmNrfeGOgk41"},"source":["MAX_LENGTH = 80 # max no. words for each sentence.\n","OVERLAP = 20    # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n","MAX_SAMPLE = 5  # set a small number for experimentation, set None for production."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Ll5AgNfbgk41"},"source":["train_path = '/content/gdrive/MyDrive/Data_Coleridge Initiative/train.csv'\n","paper_train_folder = '/content/gdrive/MyDrive/Data_Coleridge Initiative/train'\n","\n","train = pd.read_csv(train_path)\n","train = train[:MAX_SAMPLE]\n","print(f'No. raw training rows: {len(train)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ekQdUt57gk42"},"source":["train = train.groupby('Id').agg({\n","    'pub_title': 'first',\n","    'dataset_title': '|'.join,\n","    'dataset_label': '|'.join,\n","    'cleaned_label': '|'.join\n","}).reset_index()\n","\n","print(f'No. grouped training rows: {len(train)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"O29ZLiX6gk42"},"source":["papers = {}\n","for paper_id in train['Id'].unique():\n","    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n","        paper = json.load(f)\n","        papers[paper_id] = paper"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jas9dZ4Rgk43"},"source":["def clean_training_text(txt):\n","    \"\"\"\n","    similar to the default clean_text function but without lowercasing.\n","    \"\"\"\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n","\n","def shorten_sentences(sentences):\n","    short_sentences = []\n","    for sentence in sentences:\n","        words = sentence.split()\n","        if len(words) > MAX_LENGTH:\n","            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n","                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n","        else:\n","            short_sentences.append(sentence)\n","    return short_sentences\n","\n","def find_sublist(big_list, small_list):\n","    all_positions = []\n","    for i in range(len(big_list) - len(small_list) + 1):\n","        if small_list == big_list[i:i+len(small_list)]:\n","            all_positions.append(i)\n","    \n","    return all_positions\n","\n","def tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n","    sentence_words = sentence.split()\n","    \n","    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n","                                  for label in labels): # positive sample\n","        nes = ['O'] * len(sentence_words)\n","        for label in labels:\n","            label_words = label.split()\n","\n","            all_pos = find_sublist(sentence_words, label_words)\n","            for pos in all_pos:\n","                nes[pos] = 'B'\n","                for i in range(pos+1, pos+len(label_words)):\n","                    nes[i] = 'I'\n","\n","        return True, list(zip(sentence_words, nes))\n","        \n","    else: # negative sample\n","        nes = ['O'] * len(sentence_words)\n","        return False, list(zip(sentence_words, nes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-5d8iYi3gk44"},"source":["cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n","ner_data = []\n","\n","pbar = tqdm(total=len(train))\n","for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n","    # paper\n","    paper = papers[id]\n","    \n","    # labels\n","    labels = dataset_label.split('|')\n","    labels = [clean_training_text(label) for label in labels]\n","    \n","    # sentences\n","    sentences = set([clean_training_text(sentence) for section in paper \n","                 for sentence in section['text'].split('.') \n","                ])\n","    sentences = shorten_sentences(sentences) # make sentences short\n","    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n","    \n","    # positive sample\n","    for sentence in sentences:\n","        is_positive, tags = tag_sentence(sentence, labels)\n","        if is_positive:\n","            cnt_pos += 1\n","            ner_data.append(tags)\n","        elif any(word in sentence.lower() for word in ['data', 'study']): \n","            ner_data.append(tags)\n","            cnt_neg += 1\n","    \n","    # process bar\n","    pbar.update(1)\n","    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n","\n","# shuffling\n","random.shuffle(ner_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IGo1258ygk44"},"source":["dict =[]\n","with open('train_ner.json', 'w') as f:\n","    for row in ner_data:\n","        words, nes = list(zip(*row))\n","        row_json = {'tokens' : words, 'tags' : nes}\n","        dict.append(row_json)\n","        json.dump(row_json, f)\n","        f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Oz03AleIgk45"},"source":["data = pd.DataFrame(dict)\n","data = data.apply(pd.Series.explode).reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"pz6eqtzigk45"},"source":["data =data.fillna(method =\"ffill\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Sf2YfHZ_gk46"},"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"CDSQyzEygk46"},"source":["data.rename(columns={\"index\":\"sentence_id\",\"tokens\":\"words\",\"tags\":\"labels\"}, inplace =True)\n","data[\"labels\"] = data[\"labels\"].str.upper()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"n-UC8KVPgk46"},"source":["sample_sub = pd.read_csv('/content/gdrive/MyDrive/Data_Coleridge Initiative/sample_submission.csv')\n","test_files_path = '/content/gdrive/MyDrive/Data_Coleridge Initiative/test'\n","train_files_path = paper_train_folder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"_QoDazRcgk47"},"source":["def read_append_return(filename, train_files_path=train_files_path, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data\n","sample_sub['text'] = sample_sub['Id'].apply(partial(read_append_return, train_files_path=test_files_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"GWA4BddOgk47"},"source":["sample_sub"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"is4Yxe5jgk48"},"source":["X= data[[\"sentence_id\",\"words\"]]\n","Y =data[\"labels\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"QeGWp9vhgk48"},"source":["x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size =0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Qqf4yBjogk48"},"source":["#building up train data and test data\n","train_data = pd.DataFrame({\"sentence_id\":x_train[\"sentence_id\"],\"words\":x_train[\"words\"],\"labels\":y_train})\n","test_data = pd.DataFrame({\"sentence_id\":x_test[\"sentence_id\"],\"words\":x_test[\"words\"],\"labels\":y_test})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"OsnqmkCogk49"},"source":["from simpletransformers.ner import NERModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"UR7mN4_7gk49"},"source":["label = data[\"labels\"].unique().tolist()\n","label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ujvvuyd0gk49"},"source":["import torch\n","import torch.nn\n","cuda_available = torch.cuda.is_available()\n","cuda_available"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"zTNTHY4Cgk4-"},"source":["model = NERModel('bert', '../input/huggingface-bert/bert-base-cased',labels=label,args = {'num_train_epochs':1, 'learning_rate':1e-4,\n","'overwrite_output_dir':True, 'train_batch_size':32, 'eval_batch_size':32}, use_cuda=cuda_available)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"nKV_4PWWgk4-"},"source":["model.train_model(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Ze5oLgZlgk4-"},"source":["result, model_outputs, preds_list = model.eval_model(test_data)\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"bHPglhSygk4-"},"source":["predicts = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"N1nHxrSPgk4_"},"source":["def predict(text):\n","    predict = []\n","    orig_string = text\n","    list_of_lines = []\n","    max_length = 350\n","    while len(orig_string) > max_length:\n","        line_length = orig_string[:max_length].rfind(' ')\n","        list_of_lines.append(orig_string[:line_length])\n","        orig_string = orig_string[line_length + 1:]\n","    list_of_lines.append(orig_string)\n","    for i in tqdm(range(len(list_of_lines))):\n","        prediction, model_output = model.predict([list_of_lines[i]])\n","        prediction=prediction[0]\n","        for i in range (len(prediction)):\n","            for x in prediction[i]:\n","                if (prediction[i][x] == 'B' or prediction[i][x] == 'I'):\n","                    predict.append(x)\n","        predict = list(set(predict))\n","    predict = \" \".join(predict)\n","    return predict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"trusted":true,"id":"-k-G3oopgk4_"},"source":["for x in tqdm(range(len(sample_sub))):\n","    PredictionString = predict(sample_sub.text[x])\n","    predicts.append(PredictionString)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"M8zznczcgk4_"},"source":["predicts"],"execution_count":null,"outputs":[]}]}